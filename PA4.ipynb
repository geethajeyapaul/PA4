{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71864ee7",
   "metadata": {},
   "source": [
    "## PA4 :\n",
    "\n",
    "Group 7:\n",
    "Afaf Konkabess and Geetha Jeyapaul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71f8b5",
   "metadata": {},
   "source": [
    "## Task 1: Why does the classifier fail on the second dataset?\n",
    "\n",
    "**First dataset (Gothenburg/Paris):**\n",
    "\n",
    "- Gothenburg → always rain  \n",
    "- Paris → sun in July, rain in December  \n",
    "\n",
    "These examples are linearly separable. A linear decision boundary can separate the \"sun\" cases from the \"rain\" cases, allowing a linear classifier to perfectly classify the training data.\n",
    "\n",
    "**Second dataset (Sydney/Paris):**\n",
    "\n",
    "- Sydney → rain in July, sun in December  \n",
    "- Paris → sun in July, rain in December  \n",
    "\n",
    "This creates an XOR pattern: the weather is \"sun\" only when the city and month are different.\n",
    "\n",
    "An XOR pattern is not linearly separable, meaning that no straight line can perfectly separate the positive and negative examples. Linear classifiers such as the Perceptron and LinearSVC can only learn linear decision boundaries. Therefore, they cannot correctly classify all examples in this dataset, even when evaluated on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d669d",
   "metadata": {},
   "source": [
    "## Task 2: Running the Perceptron on the Sentiment Dataset\n",
    "\n",
    "We run the provided Perceptron implementation on the document classification task.\n",
    "The goal is to verify that the code works correctly and achieves an accuracy of approximately 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08de7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff3ac08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data\n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95d53538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = self.decision_function(X)\n",
    "        return np.array([\n",
    "            self.positive_class if s >= 0 else self.negative_class\n",
    "            for s in scores\n",
    "        ])\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"Not a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "857739dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        self.find_classes(Y)\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                score = x.dot(self.w)\n",
    "                if y * score <= 0:\n",
    "                    self.w += y * x\n",
    "\n",
    "        self.coef_ = self.w\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dce56357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.85 sec\n",
      "Accuracy: 0.7919\n"
     ]
    }
   ],
   "source": [
    "##Run the Perceptron\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    Perceptron(n_iter=20)\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "\n",
    "print(\"Training time: {:.2f} sec\".format(t1 - t0))\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea3fa9",
   "metadata": {},
   "source": [
    "The Perceptron achieves a test accuracy of approximately 0.80, which is consistent with the expected performance for this dataset.\n",
    "This indicates that the model is able to learn a reasonable linear decision boundary for the sentiment classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670ab49",
   "metadata": {},
   "source": [
    "## Task 3: Pegasos SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1bc2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PegasosSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=10, lambda_param=0.0001):\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        self.find_classes(Y)\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                t += 1\n",
    "                eta = 1.0 / (self.lambda_param * t)\n",
    "\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                if y * score < 1:\n",
    "                    self.w = (1 - eta * self.lambda_param) * self.w + eta * y * x\n",
    "                else:\n",
    "                    self.w = (1 - eta * self.lambda_param) * self.w\n",
    "\n",
    "        self.coef_ = self.w\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e67667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.14 sec\n",
      "Accuracy: 0.8351\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    PegasosSVC(n_iter=10, lambda_param=0.0001)\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "training_time = time.time() - t0\n",
    "\n",
    "test_acc = accuracy_score(Ytest, pipeline.predict(Xtest))\n",
    "\n",
    "print(\"Training time:\", round(training_time, 2), \"sec\")\n",
    "print(\"Accuracy:\", round(test_acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc644d9f",
   "metadata": {},
   "source": [
    "Pegasos achieves a test accuracy of approximately 0.83–0.84, which is higher than the Perceptron.\n",
    "\n",
    "This improvement is expected because Pegasos optimizes the regularized hinge loss objective, which provides better generalization compared to the mistake-driven updates of the Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80ce02",
   "metadata": {},
   "source": [
    "## Task 4: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad9be12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=10, lambda_param=0.0001):\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        self.find_classes(Y)\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        n = len(Ye)\n",
    "\n",
    "        for epoch in range(self.n_iter):\n",
    "\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                t += 1\n",
    "                eta = 1.0 / (self.lambda_param * t)\n",
    "\n",
    "                score = x.dot(self.w)\n",
    "                prob_factor = 1.0 / (1.0 + np.exp(y * score))\n",
    "\n",
    "                self.w = self.w - eta * (\n",
    "                    self.lambda_param * self.w - y * x * prob_factor\n",
    "            )\n",
    "\n",
    "        # ----- Compute objective after each epoch -----\n",
    "            total_loss = 0.0\n",
    "            for x, y in zip(X, Ye):\n",
    "                margin = y * x.dot(self.w)\n",
    "                total_loss += np.log(1 + np.exp(-margin))\n",
    "\n",
    "            objective = (self.lambda_param / 2) * np.dot(self.w, self.w) + total_loss / n\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Objective = {objective:.4f}\")\n",
    "\n",
    "        self.coef_ = self.w\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b01178d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Objective = 0.4028\n",
      "Epoch 2: Objective = 0.3906\n",
      "Epoch 3: Objective = 0.3883\n",
      "Epoch 4: Objective = 0.3876\n",
      "Epoch 5: Objective = 0.3874\n",
      "Epoch 6: Objective = 0.3872\n",
      "Epoch 7: Objective = 0.3871\n",
      "Epoch 8: Objective = 0.3870\n",
      "Epoch 9: Objective = 0.3869\n",
      "Epoch 10: Objective = 0.3868\n",
      "Training time: 3.76 sec\n",
      "Accuracy: 0.8321\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    LogisticRegression(n_iter=10, lambda_param=0.0001)\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "training_time = time.time() - t0\n",
    "\n",
    "test_acc = accuracy_score(Ytest, pipeline.predict(Xtest))\n",
    "\n",
    "print(\"Training time:\", round(training_time, 2), \"sec\")\n",
    "print(\"Accuracy:\", round(test_acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9438ec",
   "metadata": {},
   "source": [
    "Logistic Regression achieves a test accuracy of approximately 0.83, which is comparable to Pegasos.\n",
    "\n",
    "Both methods optimize a regularized objective function and therefore generalize better than the Perceptron. The performance similarity is expected since both models learn linear decision boundaries but use different loss functions (hinge loss vs. logistic loss).\n",
    "\n",
    "Additionally, the objective value decreases over epochs, indicating that the optimization procedure is functioning correctly and successfully minimizing the regularized log-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8be12e",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "| Model               | Test Accuracy | Training Time |\n",
    "| ------------------- | ------------- | ------------- |\n",
    "| Perceptron          | 0.7919        | 2.15 sec      |\n",
    "| Pegasos SVC         | 0.8351        | 2.25 sec      |\n",
    "| Logistic Regression | 0.8321        | 3.57 sec      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6fb77",
   "metadata": {},
   "source": [
    "Pegasos SVC achieves the highest accuracy while remaining computationally efficient. Logistic Regression provides similar accuracy but requires more training time due to the exponential computation in the logistic loss. The Perceptron is the fastest simple linear model but performs worse since it does not optimize a regularized objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768e22f",
   "metadata": {},
   "source": [
    "## Bonus Task 1 : Speed Improvements for Pegasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa62ca5b",
   "metadata": {},
   "source": [
    "1(a): BLAS Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09726a",
   "metadata": {},
   "source": [
    "To improve computational efficiency, we replace standard NumPy vector operations with optimized BLAS routines:\n",
    "\n",
    "ddot for dot products\n",
    "\n",
    "dscal for vector scaling\n",
    "\n",
    "daxpy for vector addition\n",
    "\n",
    "BLAS routines are implemented in low-level optimized C/Fortran and can provide faster dense linear algebra operations compared to pure NumPy implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c1f812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import ddot, dscal, daxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8aef0715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLASPegasosSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=10, lambda_param=0.0001):\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        self.find_classes(Y)\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                t += 1\n",
    "                eta = 1.0 / (self.lambda_param * t)\n",
    "\n",
    "                score = ddot(x, self.w)\n",
    "\n",
    "                # scale w\n",
    "                dscal((1 - eta * self.lambda_param), self.w)\n",
    "\n",
    "                if y * score < 1:\n",
    "                    daxpy(x, self.w, a=eta * y)\n",
    "\n",
    "        self.coef_ = self.w\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "931f263f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.96 sec\n",
      "Accuracy: 0.8351\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    BLASPegasosSVC(n_iter=10, lambda_param=0.0001)\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "training_time = time.time() - t0\n",
    "\n",
    "test_acc = accuracy_score(Ytest, pipeline.predict(Xtest))\n",
    "\n",
    "print(\"Training time:\", round(training_time, 2), \"sec\")\n",
    "print(\"Accuracy:\", round(test_acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee88e19",
   "metadata": {},
   "source": [
    "1(b): Sparse Pegasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da0470fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sparse_to_dense(x, w, factor):\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    return np.dot(w[x.indices], x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4a5738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparsePegasosSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=10, lambda_param=0.0001):\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        self.find_classes(Y)\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        # Prepare list for faster iteration\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            for x, y in XY:\n",
    "\n",
    "                t += 1\n",
    "                eta = 1.0 / (self.lambda_param * t)\n",
    "\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                # Regularization step\n",
    "                self.w *= (1 - eta * self.lambda_param)\n",
    "\n",
    "                if y * score < 1:\n",
    "                    add_sparse_to_dense(x, self.w, eta * y)\n",
    "\n",
    "        self.coef_ = self.w\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2196ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 2.97 sec\n",
      "Accuracy: 0.8292\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    SparsePegasosSVC(n_iter=10, lambda_param=0.0001)\n",
    ")\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "training_time = time.time() - t0\n",
    "\n",
    "test_acc = accuracy_score(Ytest, pipeline.predict(Xtest))\n",
    "\n",
    "print(\"Training time:\", round(training_time, 2), \"sec\")\n",
    "print(\"Accuracy:\", round(test_acc, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52771f5b",
   "metadata": {},
   "source": [
    "1(c): Scaling Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d3979ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledPegasosSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=10, lambda_param=0.0001):\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        self.find_classes(Y)\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.scale = 1.0\n",
    "        t = 0\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                t += 1\n",
    "                eta = 1.0 / (self.lambda_param * t)\n",
    "\n",
    "                # compute score using scaled weights\n",
    "                score = self.scale * x.dot(self.w)\n",
    "\n",
    "                # update scale instead of w\n",
    "                self.scale *= (1 - eta * self.lambda_param)\n",
    "\n",
    "                # avoid numerical underflow\n",
    "                if self.scale < 1e-8:\n",
    "                    self.w *= self.scale\n",
    "                    self.scale = 1.0\n",
    "\n",
    "                if y * score < 1:\n",
    "                    self.w += (eta * y / self.scale) * x\n",
    "\n",
    "        # apply final scaling\n",
    "        self.w *= self.scale\n",
    "\n",
    "        self.coef_ = self.w\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9152c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.79 sec\n",
      "Accuracy: 0.8351\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    ScaledPegasosSVC(n_iter=10, lambda_param=0.0001)\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "training_time = time.time() - t0\n",
    "\n",
    "test_acc = accuracy_score(Ytest, pipeline.predict(Xtest))\n",
    "\n",
    "print(\"Training time:\", round(training_time, 2), \"sec\")\n",
    "print(\"Accuracy:\", round(test_acc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6d4e5",
   "metadata": {},
   "source": [
    "Bonus 1 – Speed Comparison\n",
    "\n",
    "| Version            | Accuracy | Training Time |\n",
    "| ------------------ | -------- | ------------- |\n",
    "| Pegasos (baseline) | 0.8351   | 2.25 sec      |\n",
    "| BLAS Pegasos       | 0.8351   | ~2.1–2.5 sec  |\n",
    "| Sparse Pegasos     | 0.8292   | 3.37 sec      |\n",
    "| Scaled Pegasos     | 0.8351   | 1.97 sec      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f14a59",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "Among the investigated optimizations, the scaling trick provides the most consistent improvement. By avoiding repeated full-vector scaling operations, it reduces computational overhead while preserving accuracy.\n",
    "\n",
    "The BLAS implementation does not yield significant gains due to the relatively small feature dimensionality. The sparse implementation is slower in this setting because the dimensionality was reduced to 1000 features, making dense operations already efficient.\n",
    "\n",
    "Overall, the scaling trick is the most effective optimization for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
